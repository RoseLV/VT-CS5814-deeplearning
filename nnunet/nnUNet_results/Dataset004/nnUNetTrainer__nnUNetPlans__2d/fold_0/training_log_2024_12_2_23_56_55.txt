
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2024-12-02 23:56:55.052510: do_dummy_2d_data_aug: False 
2024-12-02 23:56:55.056267: Creating new 5-fold cross-validation split... 
2024-12-02 23:56:55.061864: Desired fold for training: 0 
2024-12-02 23:56:55.061908: This split has 1098 training and 275 validation cases. 
2024-12-02 23:56:58.081307: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 49, 'patch_size': [256, 256], 'median_image_size_in_voxels': [254.0, 254.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset004', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 254, 254], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 195.0, 'mean': 54.518856048583984, 'median': 51.0, 'min': 0.0, 'percentile_00_5': 6.0, 'percentile_99_5': 154.0, 'std': 23.96947479248047}, '1': {'max': 233.0, 'mean': 82.68402862548828, 'median': 80.0, 'min': 0.0, 'percentile_00_5': 12.0, 'percentile_99_5': 169.0, 'std': 30.953144073486328}, '2': {'max': 247.0, 'mean': 53.25243377685547, 'median': 50.0, 'min': 1.0, 'percentile_00_5': 7.0, 'percentile_99_5': 156.0, 'std': 23.888364791870117}}} 
 
2024-12-02 23:56:58.733715: unpacking dataset... 
2024-12-02 23:57:03.700061: unpacking done... 
2024-12-02 23:57:03.705001: Unable to plot network architecture: nnUNet_compile is enabled! 
2024-12-02 23:57:03.709805:  
2024-12-02 23:57:03.710069: Epoch 0 
2024-12-02 23:57:03.710144: Current learning rate: 0.01 
2024-12-02 23:57:57.726342: train_loss -0.6458 
2024-12-02 23:57:57.726444: val_loss -0.8284 
2024-12-02 23:57:57.726493: Pseudo dice [np.float32(0.8754)] 
2024-12-02 23:57:57.726545: Epoch time: 54.02 s 
2024-12-02 23:57:57.726589: Yayy! New best EMA pseudo Dice: 0.8754000067710876 
2024-12-02 23:57:58.846575:  
2024-12-02 23:57:58.846739: Epoch 1 
2024-12-02 23:57:58.846841: Current learning rate: 0.00999 
2024-12-02 23:58:38.374034: train_loss -0.8442 
2024-12-02 23:58:38.374142: val_loss -0.8654 
2024-12-02 23:58:38.374190: Pseudo dice [np.float32(0.9006)] 
2024-12-02 23:58:38.374243: Epoch time: 39.53 s 
2024-12-02 23:58:38.374286: Yayy! New best EMA pseudo Dice: 0.8779000043869019 
2024-12-02 23:58:41.503800:  
2024-12-02 23:58:41.504081: Epoch 2 
2024-12-02 23:58:41.504153: Current learning rate: 0.00998 
2024-12-02 23:59:21.136128: train_loss -0.8715 
2024-12-02 23:59:21.136235: val_loss -0.8778 
2024-12-02 23:59:21.136286: Pseudo dice [np.float32(0.9095)] 
2024-12-02 23:59:21.136361: Epoch time: 39.63 s 
2024-12-02 23:59:21.136407: Yayy! New best EMA pseudo Dice: 0.8810999989509583 
2024-12-02 23:59:24.364434:  
2024-12-02 23:59:24.364565: Epoch 3 
2024-12-02 23:59:24.364636: Current learning rate: 0.00997 
2024-12-03 00:00:04.096230: train_loss -0.8822 
2024-12-03 00:00:04.096332: val_loss -0.8834 
2024-12-03 00:00:04.096382: Pseudo dice [np.float32(0.9138)] 
2024-12-03 00:00:04.096436: Epoch time: 39.73 s 
2024-12-03 00:00:04.096481: Yayy! New best EMA pseudo Dice: 0.8842999935150146 
2024-12-03 00:00:07.845214:  
2024-12-03 00:00:07.845348: Epoch 4 
2024-12-03 00:00:07.845424: Current learning rate: 0.00996 
2024-12-03 00:00:47.596243: train_loss -0.8912 
2024-12-03 00:00:47.596371: val_loss -0.8898 
2024-12-03 00:00:47.596420: Pseudo dice [np.float32(0.9183)] 
2024-12-03 00:00:47.596476: Epoch time: 39.75 s 
2024-12-03 00:00:47.596521: Yayy! New best EMA pseudo Dice: 0.8877000212669373 
2024-12-03 00:00:50.822107:  
2024-12-03 00:00:50.822234: Epoch 5 
2024-12-03 00:00:50.822308: Current learning rate: 0.00995 
2024-12-03 00:01:30.652651: train_loss -0.8958 
2024-12-03 00:01:30.652755: val_loss -0.8887 
2024-12-03 00:01:30.652806: Pseudo dice [np.float32(0.9172)] 
2024-12-03 00:01:30.652861: Epoch time: 39.83 s 
2024-12-03 00:01:30.652960: Yayy! New best EMA pseudo Dice: 0.8906999826431274 
2024-12-03 00:01:33.937424:  
2024-12-03 00:01:33.937559: Epoch 6 
2024-12-03 00:01:33.937632: Current learning rate: 0.00995 
2024-12-03 00:02:13.807848: train_loss -0.8991 
2024-12-03 00:02:13.808029: val_loss -0.8904 
2024-12-03 00:02:13.808155: Pseudo dice [np.float32(0.918)] 
2024-12-03 00:02:13.808271: Epoch time: 39.87 s 
2024-12-03 00:02:13.808332: Yayy! New best EMA pseudo Dice: 0.8934000134468079 
2024-12-03 00:02:17.111513:  
2024-12-03 00:02:17.111641: Epoch 7 
2024-12-03 00:02:17.111711: Current learning rate: 0.00994 
2024-12-03 00:02:56.963843: train_loss -0.9031 
2024-12-03 00:02:56.964004: val_loss -0.8928 
2024-12-03 00:02:56.964051: Pseudo dice [np.float32(0.9198)] 
2024-12-03 00:02:56.964105: Epoch time: 39.85 s 
2024-12-03 00:02:56.964196: Yayy! New best EMA pseudo Dice: 0.8960000276565552 
2024-12-03 00:03:00.045996:  
2024-12-03 00:03:00.046123: Epoch 8 
2024-12-03 00:03:00.046194: Current learning rate: 0.00993 
2024-12-03 00:03:39.891966: train_loss -0.9087 
2024-12-03 00:03:39.892077: val_loss -0.8993 
2024-12-03 00:03:39.892130: Pseudo dice [np.float32(0.9251)] 
2024-12-03 00:03:39.892187: Epoch time: 39.85 s 
2024-12-03 00:03:39.892234: Yayy! New best EMA pseudo Dice: 0.8989999890327454 
2024-12-03 00:03:43.044385:  
2024-12-03 00:03:43.044565: Epoch 9 
2024-12-03 00:03:43.044638: Current learning rate: 0.00992 
2024-12-03 00:04:22.912343: train_loss -0.9116 
2024-12-03 00:04:22.912526: val_loss -0.8975 
2024-12-03 00:04:22.912575: Pseudo dice [np.float32(0.9237)] 
2024-12-03 00:04:22.912627: Epoch time: 39.87 s 
2024-12-03 00:04:22.912669: Yayy! New best EMA pseudo Dice: 0.9014000296592712 
2024-12-03 00:04:26.207617:  
2024-12-03 00:04:26.207785: Epoch 10 
2024-12-03 00:04:26.207855: Current learning rate: 0.00991 
2024-12-03 00:05:06.067189: train_loss -0.9128 
2024-12-03 00:05:06.067317: val_loss -0.8943 
2024-12-03 00:05:06.067389: Pseudo dice [np.float32(0.9209)] 
2024-12-03 00:05:06.067447: Epoch time: 39.86 s 
2024-12-03 00:05:06.067521: Yayy! New best EMA pseudo Dice: 0.9034000039100647 
2024-12-03 00:05:09.543916:  
2024-12-03 00:05:09.544082: Epoch 11 
2024-12-03 00:05:09.544151: Current learning rate: 0.0099 
2024-12-03 00:05:49.375160: train_loss -0.9134 
2024-12-03 00:05:49.375286: val_loss -0.8979 
2024-12-03 00:05:49.375335: Pseudo dice [np.float32(0.9232)] 
2024-12-03 00:05:49.375388: Epoch time: 39.83 s 
2024-12-03 00:05:49.375432: Yayy! New best EMA pseudo Dice: 0.9053999781608582 
2024-12-03 00:05:52.614692:  
2024-12-03 00:05:52.614817: Epoch 12 
2024-12-03 00:05:52.614890: Current learning rate: 0.00989 
2024-12-03 00:06:32.571415: train_loss -0.9167 
2024-12-03 00:06:32.571520: val_loss -0.8959 
2024-12-03 00:06:32.571595: Pseudo dice [np.float32(0.9219)] 
2024-12-03 00:06:32.571702: Epoch time: 39.96 s 
2024-12-03 00:06:32.571749: Yayy! New best EMA pseudo Dice: 0.9070000052452087 
2024-12-03 00:06:35.838568:  
2024-12-03 00:06:35.838700: Epoch 13 
2024-12-03 00:06:35.838773: Current learning rate: 0.00988 
2024-12-03 00:07:15.765576: train_loss -0.9178 
2024-12-03 00:07:15.765682: val_loss -0.8939 
2024-12-03 00:07:15.765734: Pseudo dice [np.float32(0.9203)] 
2024-12-03 00:07:15.765789: Epoch time: 39.93 s 
2024-12-03 00:07:15.765835: Yayy! New best EMA pseudo Dice: 0.9083999991416931 
2024-12-03 00:07:19.247339:  
2024-12-03 00:07:19.247471: Epoch 14 
2024-12-03 00:07:19.247542: Current learning rate: 0.00987 
2024-12-03 00:07:59.121143: train_loss -0.9211 
2024-12-03 00:07:59.121244: val_loss -0.8984 
2024-12-03 00:07:59.121329: Pseudo dice [np.float32(0.9238)] 
2024-12-03 00:07:59.121386: Epoch time: 39.87 s 
2024-12-03 00:07:59.121431: Yayy! New best EMA pseudo Dice: 0.9099000096321106 
2024-12-03 00:08:02.320972:  
2024-12-03 00:08:02.321106: Epoch 15 
2024-12-03 00:08:02.321177: Current learning rate: 0.00986 
2024-12-03 00:08:42.295376: train_loss -0.9217 
2024-12-03 00:08:42.295477: val_loss -0.8974 
2024-12-03 00:08:42.295527: Pseudo dice [np.float32(0.9236)] 
2024-12-03 00:08:42.295580: Epoch time: 39.98 s 
2024-12-03 00:08:42.295624: Yayy! New best EMA pseudo Dice: 0.911300003528595 
2024-12-03 00:08:45.465827:  
2024-12-03 00:08:45.466016: Epoch 16 
2024-12-03 00:08:45.466086: Current learning rate: 0.00986 
2024-12-03 00:09:25.382518: train_loss -0.9227 
2024-12-03 00:09:25.382626: val_loss -0.8955 
2024-12-03 00:09:25.382679: Pseudo dice [np.float32(0.9225)] 
2024-12-03 00:09:25.382734: Epoch time: 39.92 s 
2024-12-03 00:09:25.382781: Yayy! New best EMA pseudo Dice: 0.9124000072479248 
2024-12-03 00:09:28.774966:  
2024-12-03 00:09:28.775213: Epoch 17 
2024-12-03 00:09:28.775291: Current learning rate: 0.00985 
2024-12-03 00:10:08.675372: train_loss -0.9231 
2024-12-03 00:10:08.675474: val_loss -0.9003 
2024-12-03 00:10:08.675524: Pseudo dice [np.float32(0.925)] 
2024-12-03 00:10:08.675576: Epoch time: 39.9 s 
2024-12-03 00:10:08.675620: Yayy! New best EMA pseudo Dice: 0.9136999845504761 
2024-12-03 00:10:11.549581:  
2024-12-03 00:10:11.549716: Epoch 18 
2024-12-03 00:10:11.549784: Current learning rate: 0.00984 
2024-12-03 00:10:51.467244: train_loss -0.9256 
2024-12-03 00:10:51.467482: val_loss -0.9014 
2024-12-03 00:10:51.467558: Pseudo dice [np.float32(0.9269)] 
2024-12-03 00:10:51.467614: Epoch time: 39.92 s 
2024-12-03 00:10:51.467659: Yayy! New best EMA pseudo Dice: 0.9150000214576721 
2024-12-03 00:10:54.673452:  
2024-12-03 00:10:54.673613: Epoch 19 
2024-12-03 00:10:54.673710: Current learning rate: 0.00983 
2024-12-03 00:11:34.615620: train_loss -0.9269 
2024-12-03 00:11:34.615725: val_loss -0.8991 
2024-12-03 00:11:34.615775: Pseudo dice [np.float32(0.9251)] 
2024-12-03 00:11:34.615857: Epoch time: 39.94 s 
2024-12-03 00:11:34.615992: Yayy! New best EMA pseudo Dice: 0.9160000085830688 
2024-12-03 00:11:37.784043:  
2024-12-03 00:11:37.784163: Epoch 20 
2024-12-03 00:11:37.784234: Current learning rate: 0.00982 
2024-12-03 00:12:17.703577: train_loss -0.9279 
2024-12-03 00:12:17.703707: val_loss -0.8943 
2024-12-03 00:12:17.703758: Pseudo dice [np.float32(0.9207)] 
2024-12-03 00:12:17.703813: Epoch time: 39.92 s 
2024-12-03 00:12:17.703858: Yayy! New best EMA pseudo Dice: 0.9164999723434448 
2024-12-03 00:12:20.932358:  
2024-12-03 00:12:20.932485: Epoch 21 
2024-12-03 00:12:20.932554: Current learning rate: 0.00981 
2024-12-03 00:13:00.853225: train_loss -0.9303 
2024-12-03 00:13:00.853329: val_loss -0.9002 
2024-12-03 00:13:00.853381: Pseudo dice [np.float32(0.9252)] 
2024-12-03 00:13:00.853467: Epoch time: 39.92 s 
2024-12-03 00:13:00.853514: Yayy! New best EMA pseudo Dice: 0.9172999858856201 
2024-12-03 00:13:03.976317:  
2024-12-03 00:13:03.976469: Epoch 22 
2024-12-03 00:13:03.976542: Current learning rate: 0.0098 
2024-12-03 00:13:43.940502: train_loss -0.9314 
2024-12-03 00:13:43.940604: val_loss -0.8977 
2024-12-03 00:13:43.940652: Pseudo dice [np.float32(0.9239)] 
2024-12-03 00:13:43.940705: Epoch time: 39.97 s 
2024-12-03 00:13:43.940749: Yayy! New best EMA pseudo Dice: 0.9179999828338623 
2024-12-03 00:13:47.029450:  
2024-12-03 00:13:47.029613: Epoch 23 
2024-12-03 00:13:47.029685: Current learning rate: 0.00979 
2024-12-03 00:14:26.961396: train_loss -0.9321 
2024-12-03 00:14:26.961525: val_loss -0.8995 
2024-12-03 00:14:26.961579: Pseudo dice [np.float32(0.9247)] 
2024-12-03 00:14:26.961729: Epoch time: 39.93 s 
2024-12-03 00:14:26.961853: Yayy! New best EMA pseudo Dice: 0.9186999797821045 
2024-12-03 00:14:30.056726:  
2024-12-03 00:14:30.056859: Epoch 24 
2024-12-03 00:14:30.056982: Current learning rate: 0.00978 
2024-12-03 00:15:09.974237: train_loss -0.9299 
2024-12-03 00:15:09.974339: val_loss -0.9009 
2024-12-03 00:15:09.974388: Pseudo dice [np.float32(0.9254)] 
2024-12-03 00:15:09.974442: Epoch time: 39.92 s 
2024-12-03 00:15:09.974487: Yayy! New best EMA pseudo Dice: 0.9193000197410583 
2024-12-03 00:15:13.391060:  
2024-12-03 00:15:13.391207: Epoch 25 
2024-12-03 00:15:13.391280: Current learning rate: 0.00977 
2024-12-03 00:15:53.330448: train_loss -0.9307 
2024-12-03 00:15:53.330550: val_loss -0.9024 
2024-12-03 00:15:53.330600: Pseudo dice [np.float32(0.927)] 
2024-12-03 00:15:53.330653: Epoch time: 39.94 s 
2024-12-03 00:15:53.330696: Yayy! New best EMA pseudo Dice: 0.9200999736785889 
2024-12-03 00:15:56.412130:  
2024-12-03 00:15:56.412359: Epoch 26 
2024-12-03 00:15:56.412430: Current learning rate: 0.00977 
